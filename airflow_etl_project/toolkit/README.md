# ðŸŒ€ Airflow ETL Pipeline

Project ini adalah implementasi pipeline ETL (Extract-Transform-Load) menggunakan **Apache Airflow**, yang mengelola proses ekstraksi data dari berbagai sumber (CSV, JSON, API), menyimpannya dalam staging area (Parquet), dan kemudian memuatnya ke dalam database SQLite dan PostgreSQL (Neon DB).

---

## Struktur Folder

```
airflow_etl_project/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                 # DAG utama (etl_pipeline.py)
â”‚   â”œâ”€â”€ plugins/              # Plugin Airflow (jika ada)
â”‚   â”œâ”€â”€ data/                 # Output database SQLite
â”‚   â”œâ”€â”€ logs/                 # Log Airflow
â”‚   â”œâ”€â”€ dummy_data/           # File dummy (CSV, JSON, API)
â”‚   â”œâ”€â”€ staging/              # File hasil extract (Parquet)
â”‚   â”œâ”€â”€ .env                  # Konfigurasi environment
â”‚   â””â”€â”€ docker-compose.yaml   # Konfigurasi layanan
```

---

## Fitur Pipeline

### Extract
- CSV: `dummy_data/data.csv`
- JSON: `dummy_data/data.json`
- API: `https://jsonplaceholder.typicode.com/users`
- (opsional) Database: dapat dikembangkan ke sumber eksternal

### Transform
- Semua data diubah ke format **Parquet** dan disimpan di `staging/`
- JSON & API otomatis di-*flatten* dengan `json_normalize`

### Load
- Data dimuat ke:
  - **SQLite** (`data/combined_data.db`)
  - **PostgreSQL** (NeonDB)